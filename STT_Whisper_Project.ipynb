{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f9122be9-2565-4545-b650-8b3e8145f9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f43bc643-666f-4018-8371-3199b161c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"openai/whisper-small\"\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ffc0f150-bbc7-4f30-8022-e6358e2407ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(file_path):\n",
    "    audio, sr = librosa.load(file_path, sr=16000)  # Load and resample to 16kHz\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ff802b38-c40b-4672-a156-97c274eab683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio(audio, chunk_length=30):\n",
    "    sr = 16000  # Sample rate\n",
    "    chunk_size = chunk_length * sr  # 30 seconds * sample rate\n",
    "    chunks = [audio[i:i + chunk_size] for i in range(0, len(audio), chunk_size)]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6afb4fae-ca18-432f-8c27-202a3513232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio):\n",
    "    inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    predicted_ids = model.generate(inputs.input_features)\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "    return transcription[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cae5ec8c-0378-46c6-b1e4-7ec6ef27d5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_long_audio(file_path):\n",
    "    # Load the audio\n",
    "    audio = load_audio(file_path)\n",
    "    \n",
    "    # Split the audio into chunks\n",
    "    chunks = split_audio(audio)\n",
    "    \n",
    "    # Transcribe each chunk\n",
    "    full_transcription = \"\"\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Transcribing chunk {i + 1} of {len(chunks)}...\")\n",
    "        transcription = transcribe_audio(chunk)\n",
    "        full_transcription += transcription + \" \"\n",
    "    \n",
    "    return full_transcription.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "51d8b751-380b-42b5-b1e1-aa734baaf74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this path with the path to your MP3 file\n",
    "audio_path = \"C:\\\\Users\\\\bibha\\\\Downloads\\\\STT_Whisper_audio (online-audio-converter.com).mp3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e81acc70-1cc7-492c-b6a5-63f689069bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing chunk 1 of 3...\n",
      "Transcribing chunk 2 of 3...\n",
      "Transcribing chunk 3 of 3...\n",
      "Full Transcription: Hello everyone, my name is Biba Kumari. My project name is STT Whisper. This project demonstrates how to use Whisper's model via Hugging Phase Transformers to convert MP3 audio files into text implemented in Python using a Jupyter notebook for ease of understanding and experimentation. It includes steps for loading and pre-processing audio files, transcribing them using the Whisper model and evaluating the model's  performance on custom audio inputs. The whisper model supports the transcription and can automatically detect the language of the audio, making it versatile for applications like transcribing meetings, generating subtitles, or aiding language learning. Ideal for AI ML enthusiasts, developers, and researchers, this project provides a user-friendly well-documented implementation that can be customized for specific users.  cases such as fine tuning for domain specific tasks or integrating into applications by offering a practical and accessible approach speech-to-text technology.\n"
     ]
    }
   ],
   "source": [
    "# Transcribe the long audio file\n",
    "transcription = transcribe_long_audio(audio_path)\n",
    "print(\"Full Transcription:\", transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3bf192-dcf4-41f3-84aa-5d1f0c18387c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
